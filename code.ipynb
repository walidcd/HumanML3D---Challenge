{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237331e4-1ccc-44b4-b7e1-9dfeda910019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    motions_dir = \"motions\"\n",
    "    texts_dir = \"texts\"\n",
    "    train_list = \"train.txt\"\n",
    "    test_list = \"test.txt\"\n",
    "    val_list = \"val.txt\"\n",
    "\n",
    "    # Hyperparameter ranges for tuning \n",
    "    hidden_dims = [128, 256, 512]\n",
    "    emb_dims = [64, 128, 256]\n",
    "    enc_layers_options = [1, 2, 3]\n",
    "    dec_layers_options = [1, 2, 3]\n",
    "    dropouts = [0.2, 0.3, 0.15]\n",
    "    batch_sizes = [32, 64, 16]\n",
    "    lrs = [0.001, 0.0005, 0.00001]\n",
    "    teacher_forcing_ratios = [0.5, 0.7, 0.3, 0.2]\n",
    "\n",
    "    # Model parameters (default, can be overridden during hyperparameter tuning)\n",
    "    hidden_dim = 256\n",
    "    emb_dim = 128\n",
    "    enc_layers = 2\n",
    "    dec_layers = 2\n",
    "    dropout = 0.3\n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "    epochs = 20\n",
    "    max_seq_len = 20\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    min_word_count = 2\n",
    "\n",
    "# Text Preprocessing\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "        self.counter = Counter()\n",
    "\n",
    "    def build_vocab(self, descriptions):\n",
    "        for desc in descriptions:\n",
    "            tokens = word_tokenize(desc.lower())\n",
    "            self.counter.update(tokens)\n",
    "\n",
    "        words = [word for word, count in self.counter.items() if count >= Config.min_word_count]\n",
    "        for idx, word in enumerate(words, 4):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        return [\n",
    "            self.word2idx[token] if token in self.word2idx else self.word2idx[\"<unk>\"]\n",
    "            for token in tokens\n",
    "        ]\n",
    "\n",
    "    def denumericalize(self, numericalized):\n",
    "      return \" \".join([self.idx2word.get(idx, \"<unk>\") for idx in numericalized])\n",
    "\n",
    "# Motion Preprocessing\n",
    "class MotionNormalizer:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, motions):\n",
    "        # reshaping each motion to (frames, 22*3)\n",
    "        reshaped_motions = [m.reshape(-1, 22*3) for m in motions]\n",
    "        # Concatenating all frames\n",
    "        all_frames = np.concatenate(reshaped_motions, axis=0)\n",
    "        # For normalization ..\n",
    "        self.mean = np.mean(all_frames, axis=0)\n",
    "        self.std = np.std(all_frames, axis=0) + 1e-8\n",
    "\n",
    "    def normalize(self, motion):\n",
    "        if self.mean is None or self.std is None:\n",
    "            raise ValueError(\"Normalizer must be fitted before normalizing data\")\n",
    "        if len(motion.shape) != 2 or motion.shape[1] != len(self.mean):\n",
    "            raise ValueError(f\"Expected motion shape (frames, {len(self.mean)}), got {motion.shape}\")\n",
    "        return (motion - self.mean[None, :]) / self.std[None, :]\n",
    "\n",
    "    def apply(self, motion): # This is meant for creating the submission file\n",
    "      return (motion - self.mean[None, :]) / self.std[None, :]\n",
    "\n",
    "\n",
    "# Dataset Class\n",
    "class MotionTextDataset(Dataset):\n",
    "    def __init__(self, file_list, normalizer, vocab=None, mode=\"train\"):\n",
    "        self.file_ids = self._load_file_ids(file_list)\n",
    "        self.normalizer = normalizer\n",
    "        self.motions = []\n",
    "        self.texts_list = [] # Store list of texts for BLEU calculation\n",
    "        self.mode = mode\n",
    "\n",
    "        # loading all motions to fit normalizer\n",
    "        raw_motions = []\n",
    "        print(f\"Loading {mode} motions for normalization...\")\n",
    "        for fid in tqdm(self.file_ids):\n",
    "            motion = np.load(os.path.join(Config.motions_dir, f\"{fid}.npy\"))\n",
    "            raw_motions.append(motion)\n",
    "\n",
    "\n",
    "        if mode == \"train\":\n",
    "            print(\"Fitting normalizer...\")\n",
    "            self.normalizer.fit(raw_motions)\n",
    "\n",
    "        # Process motions and texts\n",
    "        print(f\"Processing {mode} data...\")\n",
    "        for fid, motion in tqdm(zip(self.file_ids, raw_motions)):\n",
    "            # Normalize and flatten motion\n",
    "            motion = motion.reshape(-1, 22*3)\n",
    "            motion = self.normalizer.normalize(motion)\n",
    "            self.motions.append(torch.FloatTensor(motion))\n",
    "\n",
    "            # Load texts\n",
    "            if mode != \"test\":\n",
    "                with open(os.path.join(Config.texts_dir, f\"{fid}.txt\")) as f:\n",
    "                    texts = [line.split('#')[0].strip() for line in f.readlines()]\n",
    "                    self.texts_list.append(texts) # Store all texts\n",
    "                    if mode == 'train' or mode == 'val':\n",
    "                        self.texts = self.texts_list # For training/val, use self.texts for random choice in __getitem__\n",
    "                    elif mode == 'test':\n",
    "                        self.texts = None # Test doesn't need self.texts\n",
    "\n",
    "        # Build vocab from training data\n",
    "        if mode == \"train\" and vocab is None:\n",
    "            print(\"Building vocabulary...\")\n",
    "            all_texts = [t for texts in self.texts_list for t in texts]\n",
    "            self.vocab = Vocabulary()\n",
    "            self.vocab.build_vocab(all_texts)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "    def _load_file_ids(self, file_list):\n",
    "        with open(file_list) as f:\n",
    "            return [line.strip().split('.')[0] for line in f.readlines()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        motion = self.motions[idx]\n",
    "        if self.mode == \"test\":\n",
    "          return motion, self.file_ids[idx]\n",
    "        text_options = self.texts_list[idx] # Get all text options for BLEU in val/test\n",
    "        text = random.choice(text_options) # Random choice for training\n",
    "        numericalized = [self.vocab.word2idx[\"<sos>\"]] + \\\n",
    "                        self.vocab.numericalize(text) + \\\n",
    "                        [self.vocab.word2idx[\"<eos>\"]]\n",
    "        return motion, torch.LongTensor(numericalized), text_options # Return text options\n",
    "\n",
    "\n",
    "# Collate Function\n",
    "def collate_fn(batch):\n",
    "    if isinstance(batch[0][1], str): # For test data, no texts\n",
    "      motions, file_ids = zip(*batch)\n",
    "\n",
    "      # Pad motions\n",
    "      motion_lens = [len(m) for m in motions]\n",
    "      max_motion_len = max(motion_lens)\n",
    "      padded_motions = torch.zeros(len(motions), max_motion_len, motions[0].shape[1])\n",
    "      for i, m in enumerate(motions):\n",
    "        padded_motions[i, :len(m)] = m\n",
    "      return padded_motions, file_ids\n",
    "    else: # For train/val data with texts\n",
    "      motions, texts, text_options_list = zip(*batch) # Unpack text_options_list\n",
    "\n",
    "      # Pad motions\n",
    "      motion_lens = [len(m) for m in motions]\n",
    "      max_motion_len = max(motion_lens)\n",
    "      padded_motions = torch.zeros(len(motions), max_motion_len, motions[0].shape[1])\n",
    "      for i, m in enumerate(motions):\n",
    "          padded_motions[i, :len(m)] = m\n",
    "\n",
    "      # Pad texts\n",
    "      text_lens = [len(t) for t in texts]\n",
    "      max_text_len = max(text_lens)\n",
    "      padded_texts = torch.zeros(len(texts), max_text_len).long()\n",
    "      for i, t in enumerate(texts):\n",
    "          padded_texts[i, :len(t)] = t\n",
    "\n",
    "      return padded_motions, padded_texts, list(text_options_list) # Return text_options_list\n",
    "\n",
    "\n",
    "# Model Components\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers,\n",
    "                           dropout=dropout if n_layers > 1 else 0,\n",
    "                           bidirectional=True, batch_first=True)\n",
    "\n",
    "        # Adjust fc layer to reduce bidirectional outputs to decoder dimension\n",
    "        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, hidden_dim)  # to reduce output dimension\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, src_len, input_dim]\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(src)\n",
    "        # outputs: [batch_size, src_len, hidden_dim * 2]\n",
    "        # hidden: [n_layers * 2, batch_size, hidden_dim]\n",
    "\n",
    "        # Reduce output dimension\n",
    "        outputs = self.fc_out(outputs)  # [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        # Combine forward and backward states\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)  # [batch_size, hidden_dim * 2]\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.fc_hidden(hidden)  # [batch_size, hidden_dim]\n",
    "\n",
    "        # Reshape hidden and cell for decoder\n",
    "        hidden = hidden.unsqueeze(0).repeat(self.n_layers, 1, 1)\n",
    "\n",
    "        # Process cell state similarly\n",
    "        cell = torch.cat([cell[-2], cell[-1]], dim=1)\n",
    "        cell = self.dropout(cell)\n",
    "        cell = self.fc_hidden(cell)\n",
    "        cell = cell.unsqueeze(0).repeat(self.n_layers, 1, 1)\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim * 2, hidden_dim, n_layers,\n",
    "                           dropout=dropout if n_layers > 1 else 0,\n",
    "                           batch_first=True)\n",
    "\n",
    "        # Adjust attention to work with reduced dimensions\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, encoder_outputs, hidden, cell):\n",
    "        # input: [batch_size]\n",
    "        # encoder_outputs: [batch_size, src_len, hidden_dim]\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "\n",
    "        batch_size = input.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "\n",
    "        # Embed input\n",
    "        embedded = self.dropout(self.embedding(input))  # [batch_size, hidden_dim]\n",
    "        embedded = embedded.unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "\n",
    "        # Prepare hidden state for attention\n",
    "        hidden_for_attn = hidden[-1].unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention_input = torch.cat(\n",
    "            (hidden_for_attn.repeat(1, src_len, 1), encoder_outputs),\n",
    "            dim=2\n",
    "        )  # [batch_size, src_len, hidden_dim * 2]\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention = self.attention(attention_input)  # [batch_size, src_len, 1]\n",
    "        attention_weights = F.softmax(attention.squeeze(-1), dim=1)  # [batch_size, src_len]\n",
    "\n",
    "        # Apply attention to encoder outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        # [batch_size, 1, hidden_dim]\n",
    "\n",
    "        # Concatenate embedded input and context vector\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        # [batch_size, 1, hidden_dim * 2]\n",
    "\n",
    "        # Pass through LSTM\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "        # output: [batch_size, 1, hidden_dim]\n",
    "\n",
    "        # Prepare output\n",
    "        output = torch.cat((output.squeeze(1), context.squeeze(1)), dim=1)\n",
    "        prediction = self.fc_out(output)\n",
    "\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        max_len = trg.shape[1]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, max_len, vocab_size).to(self.device)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        # First input to decoder is <sos> token\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input, encoder_outputs, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def inference(self, src, max_len):\n",
    "        model.eval()\n",
    "        batch_size = src.shape[0]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, max_len, vocab_size).to(self.device)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        # First input to decoder is <sos> token\n",
    "        input = torch.ones(batch_size, dtype=torch.long).to(self.device) * train_dataset.vocab.word2idx[\"<sos>\"]\n",
    "\n",
    "        predicted_tokens = [[] for _ in range(batch_size)] # Initialize lists to hold predicted token indices for each instance in the batch\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input, encoder_outputs, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # Save each token to its corresponding instance\n",
    "            for batch_idx, token_idx in enumerate(top1):\n",
    "              predicted_tokens[batch_idx].append(token_idx.item())\n",
    "\n",
    "            input = top1\n",
    "\n",
    "        return predicted_tokens\n",
    "# Training Functions\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, trg, _ in tqdm(iterator, desc=\"Training\"): # Added _ to ignore text_options_list\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, vocab):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_bleu = 0\n",
    "    smoothing = SmoothingFunction().method1 # Smoothing function to avoid zero BLEU scores\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, trg, text_options_list in tqdm(iterator, desc=\"Evaluating\"): # Get text_options_list\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0)  # Turn off teacher forcing\n",
    "\n",
    "            output_tokens = model.inference(src, Config.max_seq_len) # Get predicted tokens\n",
    "            predicted_sentences = [vocab.denumericalize(tokens) for tokens in output_tokens] # Convert tokens to sentences\n",
    "            reference_sentences_list = text_options_list # Already list of sentences\n",
    "\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate BLEU score for each sentence in the batch\n",
    "            for i in range(len(predicted_sentences)):\n",
    "                reference_sentences = [word_tokenize(ref.lower()) for ref in reference_sentences_list[i]] # Tokenize references\n",
    "                candidate_sentence = word_tokenize(predicted_sentences[i].lower()) # Tokenize prediction\n",
    "                bleu_score = sentence_bleu(reference_sentences, candidate_sentence, smoothing_function=smoothing)\n",
    "                epoch_bleu += bleu_score\n",
    "\n",
    "    avg_bleu = epoch_bleu / len(iterator.dataset) # Average BLEU over dataset\n",
    "    return epoch_loss / len(iterator), avg_bleu\n",
    "\n",
    "def tune_hyperparameters():\n",
    "    best_bleu = -1.0\n",
    "    best_config = None\n",
    "\n",
    "    # Grid search over hyperparameters\n",
    "    for hidden_dim in Config.hidden_dims:\n",
    "        for emb_dim in Config.emb_dims:\n",
    "            for enc_layers in Config.enc_layers_options:\n",
    "                for dec_layers in Config.dec_layers_options:\n",
    "                    for dropout in Config.dropouts:\n",
    "                        for batch_size in Config.batch_sizes:\n",
    "                            for lr in Config.lrs:\n",
    "                                for teacher_forcing_ratio in Config.teacher_forcing_ratios:\n",
    "                                    current_config = {\n",
    "                                        'hidden_dim': hidden_dim,\n",
    "                                        'emb_dim': emb_dim,\n",
    "                                        'enc_layers': enc_layers,\n",
    "                                        'dec_layers': dec_layers,\n",
    "                                        'dropout': dropout,\n",
    "                                        'batch_size': batch_size,\n",
    "                                        'lr': lr,\n",
    "                                        'teacher_forcing_ratio': teacher_forcing_ratio,\n",
    "                                    }\n",
    "                                    print(f\"\\n--- Tuning with config: {current_config} ---\")\n",
    "\n",
    "                                    # Setup device\n",
    "                                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "                                    # Prepare data\n",
    "                                    normalizer = MotionNormalizer()\n",
    "                                    train_dataset = MotionTextDataset(Config.train_list, normalizer, mode=\"train\")\n",
    "                                    val_dataset = MotionTextDataset(Config.val_list, normalizer, vocab=train_dataset.vocab, mode=\"val\")\n",
    "\n",
    "                                    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                                            shuffle=True, collate_fn=collate_fn)\n",
    "                                    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                                          collate_fn=collate_fn)\n",
    "\n",
    "                                    # Initialize model with current hyperparameters\n",
    "                                    enc = Encoder(input_dim=66, hidden_dim=hidden_dim,\n",
    "                                                 n_layers=enc_layers, dropout=dropout)\n",
    "                                    dec = Decoder(output_dim=len(train_dataset.vocab.word2idx),\n",
    "                                                 hidden_dim=hidden_dim,\n",
    "                                                 n_layers=dec_layers,\n",
    "                                                 dropout=dropout)\n",
    "                                    model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "                                    # Optimizer and loss\n",
    "                                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                                    criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<pad>\"])\n",
    "\n",
    "                                    # Training loop (simplified - only one epoch for tuning example)\n",
    "                                    for epoch in range(1): # Reduced epochs for tuning\n",
    "                                        train_loss = train(model, train_loader, optimizer, criterion, clip=1)\n",
    "                                        val_loss, val_bleu = evaluate(model, val_loader, criterion, train_dataset.vocab)\n",
    "                                        print(f\"\\tEpoch: {epoch+1}, Val BLEU: {val_bleu:.3f}\")\n",
    "\n",
    "                                    if val_bleu > best_bleu:\n",
    "                                        best_bleu = val_bleu\n",
    "                                        best_config = current_config\n",
    "                                        print(f\"\\t--- New best BLEU: {best_bleu:.3f} with config: {best_config} ---\")\n",
    "\n",
    "    print(\"\\n--- Best Hyperparameters Found ---\")\n",
    "    print(f\"Best BLEU Score: {best_bleu:.3f}\")\n",
    "    print(f\"Best Config: {best_config}\")\n",
    "    return best_config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Option to tune hyperparameters \n",
    "    tune_hparams = True\n",
    "\n",
    "    if tune_hparams:\n",
    "        best_config = tune_hyperparameters()\n",
    "        # Update Config with best hyperparameters\n",
    "        Config.hidden_dim = best_config['hidden_dim']\n",
    "        Config.emb_dim = best_config['emb_dim']\n",
    "        Config.enc_layers = best_config['enc_layers']\n",
    "        Config.dec_layers = best_config['dec_layers']\n",
    "        Config.dropout = best_config['dropout']\n",
    "        Config.batch_size = best_config['batch_size']\n",
    "        Config.lr = best_config['lr']\n",
    "        Config.teacher_forcing_ratio = best_config['teacher_forcing_ratio']\n",
    "        print(\"\\n--- Using best config for final training ---\")\n",
    "        print(Config.__dict__) # Print config to verify\n",
    "    else:\n",
    "        print(\"\\n--- Using default config for training ---\")\n",
    "        print(Config.__dict__) # Print config to verify\n",
    "\n",
    "\n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Prepare data\n",
    "    print(\"Initializing datasets...\")\n",
    "    normalizer = MotionNormalizer()\n",
    "    train_dataset = MotionTextDataset(Config.train_list, normalizer, mode=\"train\")\n",
    "    val_dataset = MotionTextDataset(Config.val_list, normalizer, vocab=train_dataset.vocab, mode=\"val\")\n",
    "    test_dataset = MotionTextDataset(Config.test_list, normalizer, vocab=train_dataset.vocab, mode=\"test\") # Test dataset does not contain text\n",
    "\n",
    "    print(\"Creating data loaders...\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size,\n",
    "                            shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=Config.batch_size,\n",
    "                          collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Config.batch_size,\n",
    "                             shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    enc = Encoder(input_dim=66, hidden_dim=Config.hidden_dim,\n",
    "                 n_layers=Config.enc_layers, dropout=Config.dropout)\n",
    "    dec = Decoder(output_dim=len(train_dataset.vocab.word2idx),\n",
    "                 hidden_dim=Config.hidden_dim,\n",
    "                 n_layers=Config.dec_layers,\n",
    "                 dropout=Config.dropout)\n",
    "    model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=Config.lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.word2idx[\"<pad>\"])\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(Config.epochs):\n",
    "        print(f\"\\nEpoch: {epoch+1}/{Config.epochs}\")\n",
    "\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, clip=1)\n",
    "        val_loss, val_bleu = evaluate(model, val_loader, criterion, train_dataset.vocab) # Get BLEU score\n",
    "\n",
    "        print(f\"\\tTrain Loss: {train_loss:.3f}\")\n",
    "        print(f\"\\tVal Loss: {val_loss:.3f}\")\n",
    "        print(f\"\\tVal BLEU: {val_bleu:.3f}\") # Print BLEU score\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"\\tSaving best model...\")\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "    # Generate submission file\n",
    "    print(\"Generating submission file...\")\n",
    "    model.eval()\n",
    "\n",
    "    submission_data = []\n",
    "    with torch.no_grad():\n",
    "        for src, file_ids in tqdm(test_loader, desc=\"Generating predictions\"):\n",
    "            src = src.to(device)\n",
    "\n",
    "            predicted_tokens = model.inference(src, Config.max_seq_len)\n",
    "\n",
    "            for tokens, file_id in zip(predicted_tokens, file_ids):\n",
    "              predicted_text = train_dataset.vocab.denumericalize(tokens)\n",
    "              # remove eos and pad tokens\n",
    "              predicted_text = predicted_text.replace(\"<eos>\", \"\").replace(\"<pad>\", \"\").strip()\n",
    "              submission_data.append({\"id\": file_id, \"text\": predicted_text})\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df.to_csv(\"./submission.csv\", index=False)\n",
    "    print(\"Submission file saved to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
